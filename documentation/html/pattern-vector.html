<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
    <title>pattern-vector</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <link type="text/css" rel="stylesheet" href="../clips.css" />
    <style>
        /* Small fixes because we omit the online layout.css. */
        h3 { line-height: 1.3em; }
        #page { margin-left: auto; margin-right: auto; }
        #header, #header-inner { height: 175px; }
        #header { border-bottom: 1px solid #C6D4DD;  }
        table { border-collapse: collapse; }
    </style>
    <link href="../js/shCore.css" rel="stylesheet" type="text/css" />
    <link href="../js/shThemeDefault.css" rel="stylesheet" type="text/css" />
    <script language="javascript" src="../js/shCore.js"></script>
    <script language="javascript" src="../js/shBrushXml.js"></script>
    <script language="javascript" src="../js/shBrushJScript.js"></script>
    <script language="javascript" src="../js/shBrushPython.js"></script>
</head>
<body class="node-type-page one-sidebar sidebar-right section-pages">
    <div id="page">
    <div id="page-inner">
    <div id="header"><div id="header-inner"></div></div>
    <div id="content">
    <div id="content-inner">
    <div class="node node-type-page"
        <div class="node-inner">
        <div class="breadcrumb">View online at: <a href="http://www.clips.ua.ac.be/pages/pattern-vector" class="noexternal" target="_blank">http://www.clips.ua.ac.be/pages/pattern-vector</a></div>
        <h1>pattern.vector</h1>
        <!-- Parsed from the online documentation. -->
        <div id="node-1377" class="node node-type-page"><div class="node-inner">
<div class="content">
<p><span class="big">The pattern.vector module contains tools to count words in a "document"&nbsp;</span><span class="big">(e.g., a paragraph or a web page)</span><span class="big"> and compute <em>tf-idf</em>, <em>cosine similarity</em> and&nbsp;<em>latent semantic analysis</em> to discover keywords, search &amp; compare similar documents, cluster documents into groups, or to make predictions about other documents.</span></p>
<p>It can be used by itself or with other <a href="pattern.html">pattern</a> modules: <a href="pattern-web.html">web</a> | <a href="pattern-db.html">db</a> | <a href="pattern-en.html">en</a> | <a href="pattern-search.html">search</a> <span class="blue">&nbsp;</span>| vector | <a href="pattern-graph.html">graph</a>.</p>
<p><img src="../g/pattern_schema.gif" alt="" width="620" height="180" /></p>
<hr />
<h2>Documentation</h2>
<ul>
<li><a href="#wordcount">Word count</a></li>
<li><a href="#tf-idf">TF-IDF<span class="smallcaps"> </span></a></li>
<li><a href="#document">Document</a></li>
<li><a href="#corpus">Corpus</a></li>
<li><a href="#lsa">Latent Semantic Analysis</a></li>
<li><a href="#cluster">Clustering</a>&nbsp;<span class="smallcaps link-maintenance">(k-means, hierarchical)</span></li>
<li><a href="#classification">Classification</a>&nbsp;<span class="smallcaps link-maintenance">(naive bayes, knn, svm)</span></li>
</ul>
<p>&nbsp;</p>
<hr />
<h2><a name="wordcount"></a>Word count</h2>
<p>One way to measure the importance of a word in a text is by counting the number of times each word appears in the text (called <em>term frequency</em>). Different texts can then be compared. If a word appears frequently in many texts (<em>document frequency</em>) its importance diminishes. For example, if the word <em>important</em> occurs in many texts, it is not particularly important / unique / relevant.</p>
<p>The <span class="inline_code">words()</span> and <span class="inline_code">count()</span> functions can be used to count words in a given string:</p>
<pre class="brush:python; gutter:false; light:true;">words(string, 
       filter = lambda w: w.isalpha() and len(w) &gt; 1,
  punctuation = &#39;[]():;,.!?\n\r\t\f &#39;)
</pre><pre class="brush:python; gutter:false; light:true;">count(
      words = [], 
        top = None,         # Filter words not in the top most frequent.
  threshold = 0,            # Filter words whose count falls below threshold.
    stemmer = None,         # PORTER | LEMMA | function | None
    exclude = [],           # Filter words in the exclude list.
  stopwords = False)        # Include stop words?
</pre><ul>
<li><span class="inline_code">words()</span> returns a list of words by splitting the string on spaces.<br />Punctuation is stripped from words. If <span class="inline_code">filter(word)</span> yields <span class="inline_code">False</span>, the word is excluded.</li>
<li><span class="inline_code">count()</span> takes a list of words and returns a dictionary of <span class="inline_code">(word, count)</span>-items.</li>
</ul>
<h3>Stop words &amp; stemming</h3>
<p><a href="stop-words.html">Stop words</a> (see <span class="inline_code">pattern/vector/stopwords.txt</span>) are words that are so common (e.g. <em>each</em>, <em>his</em>, <em>very</em>) that they are ignored in <span class="inline_code">count()</span> unless parameter <span class="inline_code">stopwords</span> is set to <span class="inline_code">True</span>. There is no definite list of stop words, so you may need to tweak it to your own needs.</p>
<p>The <span class="inline_code">count()</span> function calls <span class="inline_code">stem()</span> to normalize words. For example with <span class="inline_code">PORTER</span> (<a href="http://tartarus.org/%7Emartin/PorterStemmer/">Porter2 stemming algorithm</a>),&nbsp;<em>consisted</em> and <em>consistently</em> are stemmed to <em>consist</em>. This may not always leave a real word, for example: <em>spies</em> is stemmed to <em>spi</em>. The <span class="inline_code">stemmer</span> can also be set to <span class="inline_code">LEMMA</span>. It will then call <span class="inline_code">pattern.en.singularize()</span> to guess the base form of a word, or use the word's lemma if a <span class="inline_code">Word</span> object is given. <span class="inline_code">Word</span> objects can be retrieved from a parser (<a href="pattern-en.html#parser">pattern.en.parser</a> or <a href="http://www.clips.ua.ac.be/pages/MBSP" target="_blank">MBSP</a>). This is slower than using <span class="inline_code">PORTER</span>.</p>
<pre class="brush:python; gutter:false; light:true;">stem(word, stemmer=PORTER)</pre><div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.vector import stem
&gt;&gt;&gt; print stem(&#39;spies&#39;, stemmer=PORTER)
&gt;&gt;&gt; print stem(&#39;spies&#39;, stemmer=LEMMA)

spi
spy</pre></div>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.vector import words, count, PORTER
&gt;&gt;&gt; s = &#39;The black cat was spying on the white cat.&#39;
&gt;&gt;&gt; print count(words(s), stemmer=PORTER)

{u&#39;spi&#39;: 1, u&#39;white&#39;: 1, u&#39;black&#39;: 1, u&#39;cat&#39;: 2}</pre></div>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.vector import count, LEMMA
&gt;&gt;&gt; from pattern.en import parse, Sentence
&gt;&gt;&gt; s = &#39;The black cat was spying on the white cat.&#39;
&gt;&gt;&gt; s = Sentence(parse(s, lemmata=True))
&gt;&gt;&gt; print count(s, stemmer=LEMMA)

{u&#39;spy&#39;: 1, u&#39;white&#39;: 1, u&#39;black&#39;: 1, u&#39;cat&#39;: 2}
</pre></div>
<p>&nbsp;</p>
<hr />
<h2><a name="tf-idf"></a>Term frequency – inverse document frequency</h2>
<p>Term frequency (<span class="inline_code">tf</span>) measures a word's relevancy in a single text. Document frequency (<span class="inline_code">df</span>) measures a word's overall relevancy across documents. Dividing <span class="inline_code">tf</span> by <span class="inline_code">df</span> yields <span class="inline_code">tf-idf</span>, a simple and elegant measurement of a word's uniqueness in a text when compared to other texts. For example, this can be used in a search engine to rank documents given a user query.</p>
<table class="border">
<tbody>
<tr>
<td><span class="smallcaps">Metric</span></td>
<td><span class="smallcaps">Description</span></td>
</tr>
<tr>
<td><span class="inline_code">tf</span></td>
<td>number of occurences of a word / number of words in document</td>
</tr>
<tr>
<td><span class="inline_code">df</span></td>
<td>number of documents containing a word / number of documents</td>
</tr>
<tr>
<td><span class="inline_code">idf</span></td>
<td><span class="inline_code">ln(1/df)</span></td>
</tr>
<tr>
<td><span class="inline_code">tf-idf</span></td>
<td><span class="inline_code">tf</span> <span class="inline_code">*</span> <span class="inline_code">idf</span></td>
</tr>
</tbody>
</table>
<p>The <em>document vector</em> is a list with the&nbsp;<span class="inline_code">tf-idf</span>&nbsp;value for each word in this document. Multiple documents bundled in a <em>corpus</em> form a <em>vector space</em>. By calculating the matrix dot product (= angle) of two document vectors, the similarity of the two documents can be measured. This is called <em>cosine similarity</em>.&nbsp;Let <span class="inline_code">v1</span>, <span class="inline_code">v2</span> be <span class="inline_code">Document.vector</span> objects:</p>
<p><span class="inline_code">cosθ = dot(v1, v2) / (v1.norm * v2.norm)&nbsp;</span></p>
<p>&nbsp;</p>
<hr />
<h2><a name="document"></a>Document</h2>
<p>A <span class="inline_code">Document</span> bundles the <span class="inline_code">words()</span>, <span class="inline_code">stem()</span> and <span class="inline_code">count()</span> functions with methods to determine which words are more important. The given string can also be a <span class="inline_code">Text</span>, a&nbsp;<span class="inline_code">Sentence</span>, a list of words, or a dictionary of <span class="inline_code">(word, count)</span>-items. Documents can be grouped in a <span class="inline_code">Corpus</span> to compute <span class="inline_code">tf-idf</span> and similarity.</p>
<pre class="brush:python; gutter:false; light:true;">document = Document(string, 
       filter = lambda w: w.isalpha() and len(w) &gt; 1, 
  punctuation = &#39;[]():;,.!?\n\r\t\f &#39;, 
          top = None,       # Filter words not in the top most frequent.
    threshold = 0,          # Filter words whose count falls below threshold.
      stemmer = None,       # STEMMER | LEMMA | function | None.
      exclude = [],         # Filter words in the exclude list.
    stopwords = False,      # Include stop words?
         name = None,
         type = None)</pre><pre class="brush:python; gutter:false; light:true;">document = Document.open(path, *args, **kwargs, encoding=&#39;utf-8&#39;)</pre><pre class="brush:python; gutter:false; light:true;">document.id                 # Unique number (read-only).
document.name               # Unique name, or None, used in Corpus.document().
document.type               # Document type, used with classifiers.
document.corpus             # The parent Corpus, or None.
document.terms              # Dictionary of (word, count)-items (read-only).
document.features           # List of words from Document.terms.keys(). 
document.count              # Total word count.
document.vector             # Cached tf-idf vector (read-only dict).</pre><pre class="brush:python; gutter:false; light:true;">document.tf(word)
document.tfidf(word)        # Note: simply yields tf if corpus is None.
document.keywords(top=10, normalized=True)
</pre><pre class="brush:python; gutter:false; light:true;">document.copy()</pre><ul>
<li><span class="inline_code">Document.open()</span> reads the given text file. It takes the same arguments as the constructor.<span class="inline_code"><br /></span></li>
<li><span class="inline_code">Document.tf()</span> returns the frequency of a word as a number between <span class="inline_code">0.0-1.0</span>.</li>
<li><span class="inline_code">Document.tfidf()</span> returns the word's relevancy as <span class="inline_code">tf-idf</span>.<span class="inline_code">&nbsp;</span></li>
<li><span class="inline_code">Document.keywords()</span> returns a sorted list of <span class="inline_code">(tf-idf, word)</span>-tuples.<br />With <span class="inline_code">normalized=True</span> relevancy values will be between <span class="inline_code">0.0-1.0</span> (their sum is <span class="inline_code">1.0</span>).</li>
</ul>
<p>For example:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.vector import Document
&gt;&gt;&gt; s = &#39;&#39;&#39;
&gt;&gt;&gt; The shuttle Discovery, already delayed three times by technical problems 
&gt;&gt;&gt; and bad weather, was grounded again Friday, this time by a potentially 
&gt;&gt;&gt; dangerous gaseous hydrogen leak in a vent line attached to the shipʼs 
&gt;&gt;&gt; external tank. The Discovery was initially scheduled to make its 39th 
&gt;&gt;&gt; and final flight last Monday, bearing fresh supplies and an intelligent 
&gt;&gt;&gt; robot for the International Space Station. But complications delayed the 
&gt;&gt;&gt; flight from Monday to Friday,  when the hydrogen leak led NASA to conclude 
&gt;&gt;&gt; that the shuttle would not be ready to launch before its flight window 
&gt;&gt;&gt; closed this Monday.
&gt;&gt;&gt; &#39;&#39;&#39;
&gt;&gt;&gt; d = Document(s)
&gt;&gt;&gt; print d.keywords(top=6)

[(0.15, u&#39;flight&#39;), 
 (0.15, u&#39;monday&#39;), 
 (0.10, u&#39;delay&#39;), 
 (0.10, u&#39;discovery&#39;), 
 (0.10, u&#39;friday&#39;),
 (0.10, u&#39;hydrogen&#39;)
]
</pre></div>
<p><span class="smallcaps">Document vector</span></p>
<p><span class="inline_code">Document.vector</span> is a sparse dictionary of <span class="inline_code">(word, tf-idf)</span>-items cached for performance. <br />It has a <span class="inline_code">Vector.norm</span> attribute that yields the L2-norm, used when calculating cosine similarity between documents:</p>
<p><span class="inline_code">l2 = sqrt(sum(w**2 for w in Document.vector.values()))</span></p>
<p>&nbsp;</p>
<hr />
<h2><a name="corpus"></a>Corpus</h2>
<p>A <span class="inline_code">Corpus</span> is a collection of <span class="inline_code">Document</span> objects:</p>
<pre class="brush:python; gutter:false; light:true;">corpus = Corpus(documents=[], weight=TFIDF)</pre><pre class="brush:python; gutter:false; light:true;">corpus = Corpus.build(path, *args, **kwargs)</pre><pre class="brush:python; gutter:false; light:true;">corpus = Corpus.load(path)  # Imports file created with Corpus.save().</pre><pre class="brush:python; gutter:false; light:true;">corpus.documents            # List of Documents (read-only).
corpus.document(name)       # Yields document with given name (unique).
corpus.features             # List of all Document.features. 
corpus.vector(document)     # Vector with all words from all documents.
corpus.vectors              # List of all document vectors.
corpus.features             # List of all Document.vector.keys(). 
corpus.classes              # List of all Document.type values.  
corpus.weight               # TF-IDF | TF (weight used for vectors).
corpus.density              # Overall word coverage (0.0-1.0).
corpus.lsa                  # Concept space, set with Corpus.reduce().</pre><pre class="brush:python; gutter:false; light:true;">corpus.append(document)
corpus.remove(document)
corpus.extend(documents)
corpus.clear()</pre><pre class="brush:python; gutter:false; light:true;">corpus.df(document)
corpus.idf(document)
corpus.similarity(document1, document2)
corpus.neighbors(document, top=10)
corpus.search(words=[], **kwargs)
corpus.distance(document1, document2, method=COSINE) # COSINE | EUCLIDEAN | MANHATTAN
corpus.cluster(documents=ALL, method=KMEANS)         # KMEANS | HIERARCHICAL
corpus.reduce(dimensions=NORM)                       # NORM | TOP300 | int</pre><pre class="brush:python; gutter:false; light:true;">corpus.information_gain(word)                        # Entropy (predictability).
corpus.feature_selection(top=100, method=IG)         # Top original features (terms).
corpus.filter(features=[])                           # Corpus with selected features.</pre><pre class="brush:python; gutter:false; light:true;">corpus.sets(threshold=0.5)                           # Frequent word sets.</pre><pre class="brush:python; gutter:false; light:true;">corpus.save(path, update=False)
corpus.export(path, format=ORANGE)                   # ORANGE | WEKA </pre><ul>
<li><span class="inline_code">Corpus.vector()</span> returns a <span class="inline_code">Vector</span> for the given document.<br />Vector contains all words in the corpus, with <span class="inline_code">tf-idf</span>&nbsp;&gt; 0 for words that appear in the document.<span><span><br /></span></span></li>
<li><span class="inline_code">Corpus.df()</span> returns document frequency of a word as a value between <span class="inline_code">0.0-1.0</span>.</li>
<li><span class="inline_code">Corpus.idf()</span> returns the inverse document frequency (or <span class="inline_code">None</span> if the word is not in the corpus).</li>
<li><span class="inline_code">Corpus.similarity()</span> returns the cosine similarity of two <span class="inline_code">Documents</span> between <span class="inline_code">0.0-1.0</span>.<span class="inline_code"><br /></span></li>
<li><span class="inline_code">Corpus.neighbors()</span> returns a sorted list of <span class="inline_code">(similarity, Document)</span>-tuples.</li>
<li><span class="inline_code">Corpus.search()</span> returns a sorted list of <span class="inline_code">(similarity, Document)</span>-tuples, based on a list of query words. A <span class="inline_code">Document</span> is created on-the-fly for the given words, using the given optional arguments.</li>
<li><span class="inline_code">Corpus.sets()</span> returns a dictionary of <span class="inline_code">(set(words), frequency)</span>-items of word combinations with a relative frequency above the given threshold (<span class="inline_code">0.0-1.0</span>).</li>
</ul>
<p>The example below demonstrates the use of <span class="inline_code">tf-idf</span> and cosine similarity:&nbsp;</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.vector import Document, Corpus
&gt;&gt;&gt; d1 = Document(&#39;A tiger is a big yellow cat with stripes.&#39;)
&gt;&gt;&gt; d2 = Document(&#39;A lion is a big yellow cat with manes.&#39;)
&gt;&gt;&gt; d3 = Document(&#39;An elephant is a big grey animal with a slurf.&#39;)
&gt;&gt;&gt; print d1.vector
&gt;&gt;&gt;
&gt;&gt;&gt; corpus = Corpus(documents=[d1,d2,d3])
&gt;&gt;&gt; print d1.vector
&gt;&gt;&gt; print
&gt;&gt;&gt; print corpus.similarity(d1,d2) # tiger vs. lion
&gt;&gt;&gt; print corpus.similarity(d1,d3) # tiger vs. elephant

{u&#39;tiger&#39;: 0.25, u&#39;stripes&#39;: 0.25, u&#39;yellow&#39;: 0.25, u&#39;cat&#39;: 0.25}
{u&#39;tiger&#39;: 0.27, u&#39;stripes&#39;: 0.27, u&#39;yellow&#39;: 0.10, u&#39;cat&#39;: 0.10}

0.12
0.0
</pre></div>
<p>We create documents for <em>tiger</em>, <em>lion</em> and <em>elephant</em>. The first time <em>tiger</em>'s vector is printed, all the values are equal (each keyword has the same frequency, <span class="inline_code">tf</span>). However, when we group the documents in a corpus the values of keywords <em>yellow</em> and <em>cat</em> diminish because they also appear in the <em>lion</em> document (<span class="inline_code">tf-idf</span>).</p>
<p>When we compare <em>tiger</em> to <em>lion</em> and&nbsp;<em>elephant</em>, the values indicate that <em>tiger</em> is more similar to <em>lion</em>. Their similarity is still quite low, only 12%. This is because in this (theoretical) example two-thirds of the documents (<em>tiger</em> and <em>lion</em>) share most of their keywords. If we continue to add documents for other animals (e.g. "<em>A squirrel is a small rodent with a tail.</em>") the similarity will rise. In many experiments you will need lots of documents, for example 10,000 – often with a 1,000 or more relevant words per document. More data processed = more accurate similarity.</p>
<h3>Corpus import, export, cache</h3>
<ul>
<li><span class="inline_code">Corpus.build()</span> returns a new <span class="inline_code">Corpus</span> from the text files at the given path (e.g. <span class="inline_code">path='folder/*.txt"</span>). Each file is one <span class="inline_code">Document.</span>&nbsp;The <span class="inline_code">Document.name</span> can be set using the optional <span class="inline_code">name</span> parameter, which is a function that takes the filename and returns a document name. The function furthermore takes the same optional arguments as the <span class="inline_code">Document</span> constructor.</li>
</ul>
<ul>
<li><span class="inline_code">Corpus.save()</span> exports the corpus as a binary file using the Python <span class="inline_code">cPickle</span> module – including the cache of document<span class="inline_code">&nbsp;</span>vectors and cosine similarity values. Whenever <span class="inline_code">Document.vector</span> or <span class="inline_code">Corpus.similarity()</span> is called the calculations are cached for performance. With <span class="inline_code">update=True</span>, caches all possible vectors and similarities before exporting.</li>
</ul>
<ul>
<li><span class="inline_code">Corpus.load()</span> returns a <span class="inline_code">Corpus</span> from the given file created with <span class="inline_code">Corpus.save()</span>. Since words are already stemmed, and previously cached calculations can be reused, this is faster than <span class="inline_code">Corpus.build()</span>.</li>
</ul>
<ul>
<li><span class="inline_code">Corpus.export(</span>) exports the corpus as a file that can be used as input for popular machine learning software. With <span class="inline_code">ORANGE</span> it generates a tab-separated text file for <a href="http://orange.biolab.si/">Orange</a>, with <span class="inline_code">WEKA</span> it generates an ARFF text file for <a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a>.</li>
</ul>
<p>Note that when a document is added to or removed from the corpus, the entire cache is cleared (since new words will change the <span class="inline_code">tf-idf</span> values).</p>
<p>If you need to add a lot of documents (e.g., 10,000+), use <span class="inline_code">Document.extend()</span>&nbsp;for performance. Collect the documents in batch lists of, say, a 1,000, and then extend the corpus.</p>
<p>&nbsp;</p>
<hr />
<h2><a name="lsa"></a>Latent semantic analysis</h2>
<p>Latent Semantic Analysis (LSA) is a statistical machine learning method based on singular value decomposition (SVD). <span class="small"><a class="noexternal" href="http://en.wikipedia.org/wiki/Singular_value_decomposition" target="_blank">[1]</a> <a class="noexternal" href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html" target="_blank">[2]</a></span> Related terms in the corpus are grouped into "concepts".&nbsp;Documents then get a concept vector that is an approximation of their original vector – with reduced dimensionality so that cosine similarity, clustering and classification can run faster.</p>
<p>SVD requires the Python <a href="http://numpy.scipy.org/" target="_blank">NumPy</a> package (installed by default on Mac OS X). Given a matrix with document rows and term columns, SVD yields matrix <span class="inline_code">U</span>&nbsp;with document rows and concept columns, diagonal matrix <span class="inline_code">Σ</span>&nbsp;with singular values, and matrix <span class="inline_code">Vt</span> with concept rows and term columns:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">from numpy import diag, dot
from numpy.linalg import svd
u, sigma, vt = svd(matrix, full_matrices=False)
for i in range(-k,0):
    sigma[i] = 0 # Reduce k smallest singular values.
matrix = dot(u, dot(diag(sigma), vt))</pre></div>
<p><span style="font-size: 11px;"><span style="text-decoration: underline;">Reference</span>: Wilk J. (2007). http://blog.josephwilk.net/projects/latent-semantic-analysis-in-python.html</span></p>
<div class="example">The figure below outlines LSA for a <em>nice</em>-document with a vector of nouns that occur after "nice":</div>
<table class="border" border="0">
<tbody>
<tr>
<td>
<p>&nbsp;<br /><img style="display: block; margin-left: auto; margin-right: auto;" src="../g/pattern-vector-lsa1.jpg" alt="" />&nbsp;</p>
</td>
</tr>
</tbody>
</table>
<h3>LSA concept space</h3>
<p>The <span class="inline_code">Corpus.reduce()</span> method calculates SVD and stores the concept space as <span class="inline_code">Corpus.lsa</span>. Its parameter <span class="inline_code">dimensions</span> sets the number of dimensions in the concept space (see further).</p>
<p>The <span class="inline_code">Corpus.similarity()</span>, <span class="inline_code">Corpus.neighbors()</span>, <span class="inline_code">Corpus.search()</span> and <span class="inline_code">Corpus.cluster()</span> methods will then compute in LSA concept space. Set <span class="inline_code">Corpus.lsa</span> to <span class="inline_code">None</span>&nbsp;at any time to undo the reduction. Adding or removing documents in the corpus will undo the reduction as well.</p>
<pre class="brush:python; gutter:false; light:true;">lsa = LSA(corpus, k=NORM)</pre><pre class="brush:python; gutter:false; light:true;">lsa.corpus                  # Parent Corpus.
lsa.terms                   # List of words, same as Corpus.vector.keys().
lsa.concepts                # List of concepts, each a dictionary {word: weight}
lsa.vectors                 # Dictionary {Document.id: {concept_index: weight}}</pre><pre class="brush:python; gutter:false; light:true;">lsa.transform(document)</pre><p><span class="inline_code">LSA.transform()</span> takes a <span class="inline_code">Document</span> and returns its&nbsp;<span class="inline_code">Vector</span> in concept space. This is useful for loose documents that are not in the corpus – see <span class="inline_code">kNN.classify()</span>.</p>
<p>The example below shows how terms become semantically related after LSA:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; D1 = Document(&quot;The cat purrs.&quot;, name=&quot;cat1&quot;)
&gt;&gt;&gt; D2 = Document(&quot;Curiosity killed the cat.&quot;, name=&quot;cat2&quot;)
&gt;&gt;&gt; D3 = Document(&quot;The dog wags his tail.&quot;, name=&quot;dog1&quot;)
&gt;&gt;&gt; D4 = Document(&quot;The dog is happy.&quot;, name=&quot;dog2&quot;)
&gt;&gt;&gt; corpus = Corpus([D1, D2, D3, D4])
&gt;&gt;&gt; corpus.reduce(2)
&gt;&gt;&gt; 
&gt;&gt;&gt; for document in corpus:
&gt;&gt;&gt;     print
&gt;&gt;&gt;     print document.name
&gt;&gt;&gt;     for concept, w1 in corpus.lsa.vectors[document.id].items():
&gt;&gt;&gt;         for word, w2 in corpus.lsa.concepts[concept].items():
&gt;&gt;&gt;             if w1 != 0 and w2 != 0:
&gt;&gt;&gt;                 print (word, w1 * w2)
</pre></div>
<p>The corpus is reduced to two dimensions, so there are two concepts in the concept space and each document has a weight for each concept. As illustrated below, the <em>cat</em> features have been grouped together and the <em>dog</em> features have been grouped together.</p>
<table class="border">
<tbody>
<tr>
<td style="width: 12%; text-align: center;"><strong><span class="smallcaps">concept</span></strong></td>
<td style="text-align: center;"><span class="smallcaps">cat</span></td>
<td style="text-align: center;"><span class="smallcaps">curiosity</span></td>
<td style="text-align: center;"><span class="smallcaps">dog</span></td>
<td style="text-align: center;"><span class="smallcaps">happy</span></td>
<td style="text-align: center;"><span class="smallcaps">killed</span></td>
<td style="text-align: center;"><span class="smallcaps">purrs</span></td>
<td style="text-align: center;"><span class="smallcaps">tail</span></td>
<td style="text-align: center;"><span class="smallcaps">wags</span></td>
</tr>
<tr>
<td style="text-align: center;"><span class="inline_code">0</span></td>
<td style="text-align: center;"><span class="inline_code">+0.00</span></td>
<td style="text-align: center;"><span class="inline_code">+0.00</span></td>
<td style="text-align: center;"><span class="inline_code">+0.52</span></td>
<td style="text-align: center;"><span class="inline_code">+0.78</span></td>
<td style="text-align: center;"><span class="inline_code">+0.00</span></td>
<td style="text-align: center;"><span class="inline_code">+0.00</span></td>
<td style="text-align: center;"><span class="inline_code">+0.26</span></td>
<td style="text-align: center;"><span class="inline_code">+0.26</span></td>
</tr>
<tr>
<td style="text-align: center;"><span class="inline_code">1</span></td>
<td style="text-align: center;"><span class="inline_code">-0.52</span></td>
<td style="text-align: center;"><span class="inline_code">-0.26</span></td>
<td style="text-align: center;"><span class="inline_code">+0.00</span></td>
<td style="text-align: center;"><span class="inline_code">+0.00</span></td>
<td style="text-align: center;"><span class="inline_code">-0.26</span></td>
<td style="text-align: center;"><span class="inline_code">-0.78</span></td>
<td style="text-align: center;"><span class="inline_code">+0.00</span></td>
<td style="text-align: center;"><span class="inline_code">+0.00</span></td>
</tr>
</tbody>
</table>
<table class="border">
<tbody>
<tr>
<td style="width: 12%; text-align: center;"><strong><span class="smallcaps">concept</span></strong></td>
<td style="text-align: center;"><span class="smallcaps">d1</span></td>
<td style="text-align: center;"><span class="smallcaps">d2</span></td>
<td style="text-align: center;"><span class="smallcaps">d3</span></td>
<td style="text-align: center;"><span class="smallcaps">d4</span></td>
</tr>
<tr>
<td style="text-align: center;"><span class="inline_code">0</span></td>
<td style="text-align: center;"><span class="inline_code">+0.00</span></td>
<td style="text-align: center;"><span class="inline_code">+0.00</span></td>
<td style="text-align: center;"><span class="inline_code">+0.45</span></td>
<td style="text-align: center;"><span class="inline_code">+0.90</span></td>
</tr>
<tr>
<td style="text-align: center;"><span class="inline_code">1</span></td>
<td style="text-align: center;"><span class="inline_code">-0.90</span></td>
<td style="text-align: center;"><span class="inline_code">-0.45</span></td>
<td style="text-align: center;"><span class="inline_code">+0.00</span></td>
<td style="text-align: center;"><span class="inline_code">+0.00</span></td>
</tr>
</tbody>
</table>
<h3>LSA dimension reduction</h3>
<p>Dimension reduction is useful for <strong>clustering</strong>: 3,000 documents with 1,000-term vectors are intractable <span class="small"><a class="noexternal" href="http://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank">[3]</a></span> in a pure-Python clustering algorithm. However, reducing the corpus to 100-concept vectors allows you to run through <em>k</em>-means clustering in a couple of minutes.&nbsp;The main difficulty is tweaking the <span class="inline_code">dimensions</span> parameter in <span class="inline_code">Corpus.reduce()</span>. Different values produce different results. A value that is too high results in noise. A value that is too low removes important semantical meaning.</p>
<p>Possible options for parameter <span class="inline_code">dimensions</span>:</p>
<ul>
<li><span class="inline_code">NORM</span>: uses L2-norm of the singular values as the number of dimensions to remove,</li>
<li><span class="inline_code">TOP300</span>: keeps the top 300 dimensions,</li>
<li><span class="inline_code">function</span>: user-defined function, takes the list of singular values and returns an <span class="inline_code">int</span>,</li>
<li><span class="inline_code">int</span>: the number of dimensions in the concept space.</li>
</ul>
<p><span style="text-decoration: underline;">Note</span>: document vectors are stored in a sparse format (i.e., terms for which a document scores 0 are excluded). This means that even if the corpus has a 1,000 terms, each document may have no more than 5-10 terms (= a&nbsp;<em>sparse</em>&nbsp;corpus). A sparse corpus usually clusters faster than a reduced corpus. To get an idea of the average vector length:<br /><span class="inline_code" style="font-family: Courier, monospace; font-size: 12px;">sum(len(d.vector) for d in corpus.documents) / float(len(corpus))&nbsp;</span></p>
<p>&nbsp;</p>
<hr />
<h2><a name="cluster"></a>Clustering</h2>
<p>If the <span class="inline_code">Document.type</span> of each document in the corpus is known, you can do interesting things with it – for example, build a <a href="#classification">classifier</a> for uncategorized documents. If the type is unknown, you can cluster documents into semantically related categories using unsupervised machine learning methods.</p>
<p>As a metaphor, suppose you have a number of points with (<span class="inline_code">x</span>, <span class="inline_code">y</span>) coordinates (horizontal and vertical position). You could group the points into two sets according to their distance to two arbitrary points (or <em>centroids</em>).&nbsp;More centroids create more sets. The principle holds for points in three dimensions (<span class="inline_code">x</span>,&nbsp;<span class="inline_code">y</span>,&nbsp;<span class="inline_code">z</span>) or even points in&nbsp;<span class="inline_code">n</span>&nbsp;dimensions (<span class="inline_code">x</span>,&nbsp;<span class="inline_code">y</span>,&nbsp;<span class="inline_code">z</span>,&nbsp;<span class="inline_code">u</span>,&nbsp;<span class="inline_code">v</span>, ...).&nbsp;</p>
<table class="border">
<tbody>
<tr>
<td style="text-align: center;"><img style="display: block; margin-left: auto; margin-right: auto;" src="../g/pattern-vector-cluster1.jpg" alt="" width="249" height="125" /><span class="smallcaps">random points in 2d</span></td>
<td style="text-align: center;"><img style="display: block; margin-left: auto; margin-right: auto;" src="../g/pattern-vector-cluster2.jpg" alt="" width="249" height="125" /><span class="smallcaps">points by distance to centroid</span></td>
</tr>
</tbody>
</table>
<p>A <span class="inline_code">Document.vector</span> is an n-dimensional point. For vectors we'll use cosine similarity as a distance metric, so that similar documents are grouped closer together.&nbsp;The <span class="inline_code">pattern.vector</span> module implements two simple clustering algorithms based on this principle:</p>
<ul>
<li><span class="inline_code">K-MEANS</span>: clusters the corpus into <em>k</em> groups. It is quite fast (e.g., 3000 vectors with&nbsp;<span class="inline_code">n=200</span> and <span class="inline_code">k=100</span> in about 10 minutes), but it has no guarantee to find the optimal solution, since it starts with random clusters and optimizes (read: swaps elements between clusters) from there.</li>
</ul>
<ul>
<li><span class="inline_code">HIERARCHICAL</span>: clusters the corpus in a tree: the top level item is a <span class="inline_code">Cluster</span> containing <span class="inline_code">Documents</span> and other <span class="inline_code">Clusters</span>. It is slow (e.g., 3000 vectors with&nbsp;<span class="inline_code">n=6</span> in about 30 minutes), but the optimal solution is guaranteed since it starts from the bottom up, by clustering the two nearest documents step-by-step.</li>
</ul>
<pre class="brush:python; gutter:false; light:true;">clusters = Corpus.cluster(documents=ALL, method=KMEANS, k=10, iterations=10)</pre><pre class="brush:python; gutter:false; light:true;">clusters = Corpus.cluster(documents=ALL, method=HIERARCHICAL, k=1, iterations=1000)</pre><p>For example:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.vector import Document, Corpus, HIERARCHICAL
&gt;&gt;&gt; d1 = Document(&#39;Cats are independent pets.&#39;, name=&#39;cat&#39;)
&gt;&gt;&gt; d2 = Document(&#39;Dogs are trustworthy pets.&#39;, name=&#39;dog&#39;)
&gt;&gt;&gt; d3 = Document(&#39;Boxes are made of cardboard.&#39;, name=&#39;box&#39;)
&gt;&gt;&gt; corpus = Corpus((d1, d2, d3))
&gt;&gt;&gt; print corpus.cluster(method=HIERARCHICAL, k=2)

Cluster(
  Document(id=3, name=&#39;box&#39;), Cluster(
    Document(id=2, name=&#39;dog&#39;), 
    Document(id=1, name=&#39;cat&#39;)))</pre></div>
<p><span class="inline_code">Corpus.cluster()</span> has an optional parameter <span class="inline_code">documents</span>&nbsp;that is a list of documents in the corpus, so you can intermingle <em>k</em>-means and hierarchical clustering on subsets. An optional parameter <span class="inline_code">distance</span> can be <span class="inline_code">EUCLIDEAN</span>, <span class="inline_code">MANHATTAN</span> or&nbsp;<span class="inline_code">COSINE</span> (default).</p>
<h3>Faster clustering: <em>k</em>-means++ &amp; triangle inequality</h3>
<p>For <span class="inline_code">KMEANS</span>, an optional parameter <span class="inline_code">seed=RANDOM</span> can also be set to <span class="inline_code">KMPP</span>. A weakness of <em>k</em>-means clustering is that it starts out with random clusters. The <em>k</em>-means++ or <span class="inline_code">KMPP</span> algorithm uses a specialized technique to find better starting points. It is also faster.</p>
<p>For <span class="inline_code">KMEANS</span>, an optional parameter <span class="inline_code">p=0.8</span> sets the relaxation for <em>triangle inequality</em>. This is a speed optimization where&nbsp;<span class="inline_code">p=0.5</span> is stable but slow, and where&nbsp;<span class="inline_code">p=1.0</span> may yield small errors but runs a lot faster, especially for higher <span class="inline_code">k</span> and dimensionality. If all of this is still too slow you couls reduce the corpus with LSA (<span class="link-maintenance"><a href="#lsa">see above</a></span>).&nbsp;</p>
<p><span class="small"><span style="text-decoration: underline;">References</span>:&nbsp;<br />Arthur, D. (2007). <em>k-means++: the advantages of careful seeding.&nbsp;</em>SODA'07 Proceedings.<br />Elkan, C. (2003). <em>Using the Triangle Inequality to Accelerate k-Means. </em>ICML'03 Proceedings.</span></p>
<h3>Hierarchical clusters</h3>
<p>The <span class="inline_code">KMEANS</span> method returns a list in which each item is a list of semantically related <span class="inline_code">Documents</span>. The <span class="inline_code">HIERARCHICAL</span> method returns a <span class="inline_code">Cluster</span> object, which is a list of <span class="inline_code">Documents</span> and <span class="inline_code">Clusters</span> with some additional properties:</p>
<pre class="brush:python; gutter:false; light:true;">cluster = Cluster([])</pre><pre class="brush:python; gutter:false; light:true;">cluster.depth               # Returns the maximum depth of nested clusters.
cluster.flatten(depth=1000) # Returns a flat list, down to the given depth.
cluster.traverse(visit=lambda cluster: None) </pre><div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.vector import Cluster
&gt;&gt;&gt; cluster = Cluster((1, Cluster((2, Cluster((3, 4))))))
&gt;&gt;&gt; print cluster.depth
&gt;&gt;&gt; print cluster.flatten(1) 

2
[1, 2, Cluster(3, 4)] </pre></div>
<h3>Centroid</h3>
<p>To get the centroid of a list of vectors you can use the <span class="inline_code">centroid()</span> function. It returns the mean <span class="inline_code">Vector</span>. It takes a <span class="inline_code">Cluster</span>, or a list of <span class="inline_code">Cluster</span>, <span class="inline_code">Vector</span> and/or <span class="inline_code">Document</span> objects. To compute the distance between any two vectors you can use the <span class="inline_code">distance()</span> function. A common problem with clustered vectors is that the cluster is <em>unlabeled</em> (it has no decisive type like <span class="inline_code">Document.type</span>). One solution is to calculate its <span class="inline_code">centroid()</span> and assign the type of the nearest document vector(s) to the cluster, using <span class="inline_code">distance()</span>.</p>
<pre class="brush: python;gutter: false; light: true; fontsize: 100; first-line: 1; ">centroid(vectors)               # Returns the mean Vector. </pre><pre class="brush: python;gutter: false; light: true; fontsize: 100; first-line: 1; ">features(vectors)               # Returns the set of unique keys from all vectors.</pre><pre class="brush: python;gutter: false; light: true; fontsize: 100; first-line: 1; ">distance(v1, v2, method=COSINE) # COSINE | EUCLIDEAN | MANHATTAN | HAMMING</pre><p>&nbsp;</p>
<hr />
<h2><a name="classification"></a>Classification</h2>
<p>Classification is a supervised machine learning method for assigning a <span class="inline_code">Document</span> to one of a given number of categories (or <em>classes</em>).&nbsp;If you have a corpus in which the <span class="inline_code">Document.type</span> for each document is known, you can use it to predict the type of other ("unknown") documents.&nbsp;For example: a corpus of product reviews (<em>training data</em>), for which the star rating (e.g., <span class="inline_code">****</span>) of each review is known, can be used to predict the star rating for other product reviews (this is a form of <em>opinion mining</em>).</p>
<p>The <span class="inline_code">pattern.vector</span> module implements three classifiers:</p>
<ul>
<li><span class="inline_code">BAYES</span>: Naive Bayes classifier, based on Bayes' theorem of posterior probability.&nbsp;</li>
<li><span class="inline_code">KNN</span>: <em>k</em>-nearest neighbor classifier, predicts a document's type based on the type of its nearest neighbors, i.e., the <em>k</em> top results from <span class="inline_code">Corpus.neighbors()</span>.</li>
<li><span class="inline_code">SVM</span>: support vector machine, represents documents as points in a high-dimensional space separated by hyperplanes.</li>
</ul>
<pre class="brush:python; gutter:false; light:true;">classifier = Bayes(aligned=False)</pre><pre class="brush:python; gutter:false; light:true;">classifier = kNN(k=10, distance=COSINE) # COSINE | EUCLIDEAN | MANHATTAN</pre><pre class="brush:python; gutter:false; light:true;">classifier = SVM(type=CLASSIFICATION, kernel=LINEAR)</pre><p><span class="small"><span style="text-decoration: underline;">Reference</span>: Hetland, M. L. http://hetland.org/coding/python/nbayes.py</span></p>
<h3>Classifier</h3>
<p>The <span class="inline_code">Bayes</span>, <span class="inline_code">kNN</span>&nbsp;and <span class="inline_code">SVM</span> classifiers inherit from the <span class="inline_code">Classifier</span> base class:</p>
<pre class="brush:python; gutter:false; light:true;">classifier = Classifier()</pre><pre class="brush:python; gutter:false; light:true;">classifier = Classifier.load(path)</pre><pre class="brush:python; gutter:false; light:true;">classifier.train(document, type=None)
classifier.classify(document)
</pre><pre class="brush:python; gutter:false; light:true;">classifier.classes        # List of trained types.
classifier.features       # List of words.
classifier.binary         # Classifier.classes == [True,False]?
</pre><pre class="brush:python; gutter:false; light:true;">classifier.save(path)     # Pickle file, loads fast with Classifier.load() </pre><ul>
<li><span class="inline_code">Classifier.train()</span> trains the classifier with the given document of the given type.<br />A document can be a <span class="inline_code">Document</span>&nbsp;or a list of words (strings or other hashable items).<br />If no <span class="inline_code">type</span> is given, <span class="inline_code">Document.type</span> will be used instead.</li>
<li><span class="inline_code">Classifier.classify()</span> returns the predicted type for the given document&nbsp;(a <span class="inline_code">Document</span> or a list of words). If the classifier is trained on an LSA-corpus, you need to supply the return value from <span class="inline_code">Corpus.lsa.transform()</span>.</li>
</ul>
<p>For example, say we have mined a large corpus (i.e., 10,000) of movie review titles and the star rating given by the reviewer/customer. The corpus would contain such instances as:</p>
<table class="border">
<tbody>
<tr>
<td><span class="smallcaps">Review title</span></td>
<td style="text-align: center;"><span class="smallcaps">Rating</span></td>
</tr>
<tr>
<td><em>Intruiging storyline!</em></td>
<td style="text-align: center;"><span class="inline_code">*****</span></td>
</tr>
<tr>
<td><em>Such a beautiful movie</em></td>
<td style="text-align: center;"><span class="inline_code">*****</span></td>
</tr>
<tr>
<td><em>I don't get the hype......</em></td>
<td style="text-align: center;"><span class="inline_code">***</span></td>
</tr>
<tr>
<td><em>Incredible, what a heap of rubbish.</em></td>
<td style="text-align: center;"><span class="inline_code">*</span></td>
</tr>
</tbody>
</table>
<p>We can use it to create a classifier that predicts the rating of other reviews, based on word similarity:</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.vector import Document, Bayes
&gt;&gt;&gt;
&gt;&gt;&gt; b = Bayes()
&gt;&gt;&gt; r = open(&#39;reviews.txt&#39;).readlines()
&gt;&gt;&gt; r = [x.split(&#39;	&#39;) for x in r]
&gt;&gt;&gt; for review, rating in r[:10000]:
&gt;&gt;&gt;    b.train(Document(review, type=rating))
&gt;&gt;&gt;
&gt;&gt;&gt; print b.classes
&gt;&gt;&gt; print b.classify(Document(&#39;An intriguing movie!&#39;))

[&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;]
5 </pre></div>
<h3>Accuracy, precision &amp; recall</h3>
<p>Naive Bayes can be quite effective despite its simple implementation. In the above example it has an accuracy of 60%. A random guess between the five possible star ratings would only have a 20% accuracy. Moreover, 25% of the errors are off by only one (e.g., guessed&nbsp;<span class="inline_code">**</span>&nbsp;instead of <span class="inline_code">***</span>, or&nbsp;<span class="inline_code">***</span>&nbsp;instead of&nbsp;<span class="inline_code">**</span>). This results in 75% accuracy for discerning positive (4-5) from negative reviews (1-2). Not bad.</p>
<p>You can test the accuracy with the classmethod&nbsp;<span class="inline_code">Classifier.test()</span>.&nbsp;It creates a new classifier that uses 65% of the documents in the given corpus as training data and 35% as testing data. You can supply a list of&nbsp;<span class="inline_code">Document</span>&nbsp;objects or<span class="inline_code"> (wordlist,&nbsp;</span><span class="small inline_code">type</span><span class="inline_code">)</span>-tuples. The method returns an <span class="inline_code">(accuracy,&nbsp;precision,&nbsp;recall,&nbsp;F1-score)</span>-tuple with values between&nbsp;<span class="inline_code">0.0</span>-<span class="inline_code">1.0</span>.</p>
<pre class="brush:python; gutter:false; light:true;">Bayes.test(corpus=[], d=0.65, folds=1, aligned=False)</pre><pre class="brush:python; gutter:false; light:true;">kNN.test(corpus=[], d=0.65, folds=1, k=10, distance=COSINE)</pre><pre class="brush:python; gutter:false; light:true;">SVM.test(corpus=[], d=0.65, folds=1, type=CLASSIFICATION, kernel=LINEAR)</pre><div class="example">
<pre class="brush:python; gutter:false; light:true;">&gt;&gt;&gt; from pattern.vector import Document, Corpus, Bayes
&gt;&gt;&gt;
&gt;&gt;&gt; corpus = Corpus()
&gt;&gt;&gt; for review in open(&quot;reviews.txt&quot;).readlines():
&gt;&gt;&gt;     review, rating = review.split(&#39;	&#39;)
&gt;&gt;&gt;     corpus.append(Document(review, type=rating&gt;=4, stemmer=None))
&gt;&gt;&gt;
&gt;&gt;&gt; print Bayes.test(corpus, folds=10) 

(0.83, 0.84, 0.96, 0.90) </pre></div>
<p><span class="smallcaps">k-fold cross-validation</span></p>
<p>With the <span class="inline_code">folds</span> parameter &gt; 1,&nbsp;K-fold cross-validation is performed.&nbsp;For example: in 10-fold cross-validation ten separate tests are taken,&nbsp;each using a different 1/10 of the corpus as testing data. This produces reliable results.</p>
<p><span class="smallcaps">Precision &amp; recall</span></p>
<p>Accuracy is&nbsp;the percentage of correctly classified documents in the test set. Precision &amp; recall are more reliable because they take into account false positives and false negatives – see <a href="pattern-metrics.html#accuracy">pattern.metrics</a> for more information.&nbsp;Note: precision, recall and F1-score will be <span class="inline_code">None</span> unless the classifier is binary.</p>
<h3>Feature selection</h3>
<p><span class="smallcaps">Supervised</span></p>
<p>It is common for a classifier to stagnate at accurary 50-60%.&nbsp;To make it better, you may need more data. You may also need to fine-tune the training documents. This is called&nbsp;<em>feature selection</em>:</p>
<ul>
<li>Enable word lemmatization or stemming (= <em>normalization</em>),</li>
<li>Raise the word threshold (= <em>noise filtering</em>),</li>
<li>Exclude or include punctuation marks (e.g., exclamation marks, emoticons),</li>
<li>Use a domain-specific list of words to exclude (check pattern.en <span class="link-maintenance"><a href="pattern-en.html#wordlist">wordlists</a></span>),</li>
<li>Use&nbsp;<a href="pattern-en.html#parser">part-of-speech tagging</a> to filter for specific types of words,</li>
<li>Use&nbsp;<span class="inline_code">aligned=True</span> with <span class="inline_code">Bayes</span>. This will take into account the word index when training on lists of words (i.e., words need to occur at the same position when comparing documents).</li>
</ul>
<p>You can also supply dictionaries of <span class="inline_code">(word, weight)</span>-items to <span class="inline_code">Classifier.train()</span>. The weight can be word frequency + an additional rating,&nbsp;for example to emphasize words in the review title or the e-mail subject.&nbsp;</p>
<p><span class="smallcaps"><br />Unsupervised</span></p>
<p>Features (i.e., terms/words) can also be selected automatically using a statistical method. Pattern uses information gain (i.e.,&nbsp;entropy<em>)</em>.&nbsp;<span class="inline_code">Corpus.information_gain()</span>&nbsp;yields a value indicating how predictable a feature is in the corpus. This value is used in <span class="inline_code">Corpus.feature_selection()</span>&nbsp;to compute a list of the most unpredictable (or "original") features.&nbsp;</p>
<p><span class="inline_code">Corpus.filter()</span>&nbsp;returns a corpus with only the given features. If you pass the return value from <span class="inline_code">Corpus.feature_selection()</span>&nbsp;you get a new corpus with the top original terms (e.g., 100 vs. 2,500).</p>
<div class="example">
<pre class="brush:python; gutter:false; light:true;">f = corpus1.feature_selection(top=100)
corpus2 = corpus1.filter(features=f) 
</pre></div>
<p>Less terms means less computation time. However, there can be a drop in recall (there are less features). However, accuracy scores can also increase since the "noisy" features were removed.</p>
<h3><a name="svm"></a>Support vector machine</h3>
<p>The most robust classifier is <span class="inline_code">SVM</span>. It relies on the fast&nbsp;<a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" target="_blank">libsvm</a>&nbsp;C++ library. Precompiled bindings are included for 32-bit Windows and Mac OS X, and 64-bit Mac OS X and Ubuntu. These may not work on your system. In this case you need to compile the bindings from source (see the instructions in&nbsp;<span class="inline_code">pattern/vector/svm/INSTALL.txt</span>).&nbsp;</p>
<p>The SVM classifier uses&nbsp;<em>kernel</em> functions. The simplest way to divide two clusters of points in 2D is a straight line. If that is not possible, moving the points to a higher dimension (using a kernel function) may make separation easier (using <em>hyperplanes</em>).</p>
<table class="border">
<tbody>
<tr>
<td style="text-align: center;"><img style="display: block; margin-left: auto; margin-right: auto;" src="../g/pattern-vector-svm1.jpg" alt="" width="178" height="148" /><span class="smallcaps">complex in low dimension</span></td>
<td style="text-align: center;"><img style="display: block; margin-left: auto; margin-right: auto;" src="../g/pattern-vector-svm2.jpg" alt="" width="190" height="148" /><span class="smallcaps">simple in higher dimension</span></td>
</tr>
</tbody>
</table>
<pre class="brush:python; gutter:false; light:true;">classifier = SVM(type=CLASSIFICATION, kernel=LINEAR, **kwargs)</pre><p>The <span class="inline_code">SVM</span> constructor has a number of optional parameters:</p>
<table class="border">
<tbody>
<tr>
<td><span class="smallcaps">Parameter</span></td>
<td><span class="smallcaps">Value</span></td>
<td><span class="smallcaps">Description</span></td>
</tr>
<tr>
<td><span class="inline_code">type</span></td>
<td><span class="inline_code">CLASSIFICATION</span>, <span class="inline_code">REGRESSION</span></td>
<td><span class="inline_code">REGRESSION</span> returns a float value.</td>
</tr>
<tr>
<td><span class="inline_code">kernel</span></td>
<td><span class="inline_code">LINEAR</span>, <span class="inline_code">POLYNOMIAL</span>, <span class="inline_code">RADIAL</span></td>
<td>Kernel function to use.&nbsp;</td>
</tr>
<tr>
<td><span class="inline_code">degree</span></td>
<td><span class="inline_code">3</span></td>
<td>Used in <span class="inline_code">POLYNOMIAL</span> kernel.&nbsp;</td>
</tr>
<tr>
<td><span class="inline_code">gamma</span></td>
<td><span class="inline_code">1 / len(SVM.features)</span></td>
<td>Used in <span class="inline_code">POLYNOMIAL</span> and <span class="inline_code">RADIAL</span> kernel.&nbsp;</td>
</tr>
<tr>
<td><span class="inline_code">coeff0</span></td>
<td><span class="inline_code">0</span></td>
<td>Used in <span class="inline_code">POLYNOMIAL</span> kernel.&nbsp;</td>
</tr>
<tr>
<td><span class="inline_code">cost</span></td>
<td><span class="inline_code">1</span></td>
<td>Soft margin for training errors.</td>
</tr>
<tr>
<td><span class="inline_code">epsilon</span></td>
<td>o.1</td>
<td>Tolerance for termination criterion.</td>
</tr>
<tr>
<td><span class="inline_code">cache</span></td>
<td>100</td>
<td>Cache memory size in MB.&nbsp;</td>
</tr>
<tr>
<td><span class="inline_code"><span class="inline_code">probability</span></span></td>
<td class="inline_code">False</td>
<td><span class="inline_code">CLASSIFICATION</span> yields&nbsp;<span class="inline_code">(weight, class)</span>&nbsp;values.</td>
</tr>
</tbody>
</table>
<table class="border">
<tbody>
<tr>
<td><span class="smallcaps">Kernel</span></td>
<td><span class="smallcaps">Separation</span></td>
<td><span class="smallcaps">Function</span></td>
</tr>
<tr>
<td><span class="inline_code">LINEAR</span></td>
<td>straight line</td>
<td><span class="inline_code">u' * v</span></td>
</tr>
<tr>
<td><span class="inline_code">POLYNOMIAL</span></td>
<td>curved line</td>
<td><span class="inline_code">(gamma * u' * v + coef0) ** degree</span></td>
</tr>
<tr>
<td><span class="inline_code">RADIAL</span></td>
<td>curved path</td>
<td><span class="inline_code">exp(-gamma * abs(u-v) ** 2)</span></td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<hr />
<h2>See also</h2>
<ul>
<li><a href="http://orange.biolab.si/" target="_blank">Orange</a> (GPL): d<span>ata mining &amp; machine learning in Python, with a node-based GUI.</span></li>
<li><span><a href="http://pybrain.org/" target="_blank">PyBrain</a> (BSD): p</span><span>owerful machine learning algorithms in Python + C.</span></li>
<li><a href="http://www.scipy.org/" target="_blank">SciPy</a><span> (BSD): scientific computing tools for Python.</span></li>
<li><span><a href="http://scikit-learn.org/" target="_blank">scikit-learn</a> (BSD): machine learning algorithms tightly knit with numpy, scipy, matplotlib.</span></li>
</ul>
</div>
</div></div>
        </div>
    </div>
    </div>
    </div>
    </div>
    </div>
    <script>
        SyntaxHighlighter.all();
    </script>
</body>
</html>